{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "# from datasets import load_dataset\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load spaCy's language model if necessary (for example, the English model)\n",
    "nlp = spacy.blank(\"en\")  # Replace \"en\" with the appropriate language code\n",
    "\n",
    "# Load your .spacy file\n",
    "train_doc_bin = DocBin().from_disk(\"data/train.spacy\")\n",
    "dev_doc_bin = DocBin().from_disk(\"data/dev.spacy\")\n",
    "# Deserialize the docs\n",
    "train_docs = list(train_doc_bin.get_docs(nlp.vocab))\n",
    "dev_docs = list(dev_doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can work with the docs\n",
    "for doc in train_docs[2:3]:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelsCounts(docs):\n",
    "    labels = []\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            labels.append(ent.label_)\n",
    "\n",
    "    # Convert the list of labels to a NumPy array\n",
    "    labels_array = np.array(labels)\n",
    "    unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
    "    counts = dict(zip(unique_labels, counts))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabelsCounts = getLabelsCounts(train_docs)\n",
    "devLabelsCounts = getLabelsCounts(dev_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLabelsPie(LabelsCounts, name):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(LabelsCounts)))\n",
    "    patches, texts, autotexts = plt.pie(LabelsCounts.values(), labels=LabelsCounts.keys(), \n",
    "            autopct='%1.1f%%', \n",
    "            colors=colors, \n",
    "            startangle=60,\n",
    "            wedgeprops=dict(edgecolor='w'))\n",
    "    for text in texts + autotexts:\n",
    "        text.set_fontsize(9)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.\n",
    "    plt.title(name, pad=30, fontdict = {'fontsize':20, 'fontstyle' : 'oblique'})\n",
    "    plt.savefig(f\"./plots/{name}.png\", bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLabelsPie(trainLabelsCounts, \"Named entity proportions in training\")\n",
    "saveLabelsPie(devLabelsCounts, \"Named entity proportions in development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train and dev csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# categories = \n",
    "\n",
    "def format_text(text):\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Load your .spacy file\n",
    "def load_spacy_file(file_path):\n",
    "    nlp = spacy.blank(\"en\")  # replace \"en\" with your model's language if different\n",
    "    docs = DocBin().from_disk(file_path)\n",
    "    return list(docs.get_docs(nlp.vocab))\n",
    "    # return list(nlp.from_disk(file_path))\n",
    "\n",
    "# Process documents and extract entities\n",
    "# def process_docs(docs):\n",
    "#     data = []\n",
    "#     for doc in docs:\n",
    "#         text = doc.text\n",
    "#         entities = defaultdict(set)\n",
    "#         for ent in doc.ents:\n",
    "#             entities[ent.label_].add(format_text(ent.text))\n",
    "#         entities = {label: list(ents) for label, ents in entities.items()}\n",
    "#         data.append([text, entities])\n",
    "#     return data\n",
    "\n",
    "\n",
    "def process_docs(docs):\n",
    "    data = []\n",
    "    for doc in docs:\n",
    "        text = doc.text\n",
    "        # Using a dict to maintain insertion order and uniqueness\n",
    "        entities = defaultdict(dict)\n",
    "        for ent in doc.ents:\n",
    "            entities[ent.label_][format_text(ent.text)] = None  # Key is the entity, value is a placeholder\n",
    "        # Extracting the keys (unique entities) from each dictionary\n",
    "        entities = {label: list(ents.keys()) for label, ents in entities.items()}\n",
    "        data.append([text, entities])\n",
    "    return data\n",
    "\n",
    "# Convert to DataFrame\n",
    "def to_dataframe(data):\n",
    "    # Find all unique entity labels\n",
    "    all_labels = set()\n",
    "    for _, entities in data:\n",
    "        all_labels.update(entities.keys())\n",
    "    all_labels = sorted(all_labels)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_data = []\n",
    "    for text, entities in data:\n",
    "        row = [format_text(text)] + [entities.get(label, []) for label in all_labels]\n",
    "        df_data.append(row)\n",
    "\n",
    "    columns = ['sentence'] + all_labels\n",
    "    return pd.DataFrame(df_data, columns=columns)\n",
    "\n",
    "# Load data\n",
    "train_docs = load_spacy_file('data/train.spacy')\n",
    "dev_docs = load_spacy_file('data/dev.spacy')\n",
    "\n",
    "# Process documents\n",
    "train_data = process_docs(train_docs)\n",
    "dev_data = process_docs(dev_docs)\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_df = to_dataframe(train_data)\n",
    "dev_df = to_dataframe(dev_data)\n",
    "\n",
    "# Export to CSV (optional)\n",
    "train_df.to_csv('./data/raw/train_data.csv', index=False)\n",
    "dev_df.to_csv('./data/raw/dev_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/raw/train_data.csv\")\n",
    "test = pd.read_csv(\"./data/raw/dev_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = train.sample(frac=0.1, random_state=42) # random_state for reproducibility\n",
    "train = train.drop(dev.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are plainly disputed questions of facts and it is also apposite to examine the same in these proceedings.\n",
      "sentence        3) The National Human Rights Commission undert...\n",
      "CASE_NUMBER                                                    []\n",
      "COURT                                                          []\n",
      "DATE                                                           []\n",
      "GPE                                                            []\n",
      "JUDGE                                                          []\n",
      "LAWYER                                                         []\n",
      "ORG                          ['National Human Rights Commission']\n",
      "OTHER_PERSON                                                   []\n",
      "PETITIONER                                                     []\n",
      "PRECEDENT                                                      []\n",
      "PROVISION                                                      []\n",
      "RESPONDENT                                                     []\n",
      "STATUTE                                                        []\n",
      "WITNESS                                                        []\n",
      "Name: 996, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test['sentence'].iloc[900])\n",
    "print(train.iloc[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether ignoring the oral and the documentary evidence, the First Appellate Court decided that the suit property is not an ancestral property and that the settlement deed, Ex.B4 executed was valid?\n",
      "sentence        Whether ignoring the oral and the documentary ...\n",
      "CASE_NUMBER                                                    []\n",
      "COURT                                                          []\n",
      "DATE                                                           []\n",
      "GPE                                                            []\n",
      "JUDGE                                                          []\n",
      "LAWYER                                                         []\n",
      "ORG                                                            []\n",
      "OTHER_PERSON                                                   []\n",
      "PETITIONER                                                     []\n",
      "PRECEDENT                                                      []\n",
      "PROVISION                                                      []\n",
      "RESPONDENT                                                     []\n",
      "STATUTE                                                        []\n",
      "WITNESS                                                        []\n",
      "Name: 1617, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(dev['sentence'].iloc[-4])\n",
    "print(dev.iloc[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset that contains the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_entities_column(df):\n",
    "    def entities_to_string(row):\n",
    "        # Build a dictionary of non-empty entity categories\n",
    "        entities_dict = {category: entities for category, entities in row.items() if category != 'sentence' and \"entities\" not in category }\n",
    "        # Convert the dictionary to a JSON string\n",
    "        return json.dumps(entities_dict)\n",
    "\n",
    "    # Apply the function to each row and create the new column\n",
    "    df['raw_entities'] = df.apply(entities_to_string, axis=1)\n",
    "    return df\n",
    "\n",
    "def create_dict_column(df):\n",
    "    # Function to convert a JSON string to a dictionary\n",
    "    def string_to_dict(json_str):\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return {}  # Returns an empty dictionary in case of a decoding error\n",
    "\n",
    "    # Apply the function to the 'raw_entities' column to create a new dictionary column\n",
    "    df['entities_dict'] = df['raw_entities'].apply(string_to_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_raw_entities_column(train)\n",
    "dev_data = create_raw_entities_column(dev)\n",
    "test_data = create_raw_entities_column(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_dict_column(train_data)\n",
    "\n",
    "dev_data = create_dict_column(dev_data)\n",
    "\n",
    "test_data = create_dict_column(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[\\'District Magistrate, Muzaffarnagar\\']\", \"DATE\": \"[\\'13/14.1.1999\\']\", \"GPE\": \"[]\", \"JUDGE\": \"[]\", \"LAWYER\": \"[]\", \"ORG\": \"[]\", \"OTHER_PERSON\": \"[]\", \"PETITIONER\": \"[]\", \"PRECEDENT\": \"[]\", \"PROVISION\": \"[]\", \"RESPONDENT\": \"[]\", \"STATUTE\": \"[]\", \"WITNESS\": \"[]\"}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data['raw_entities'].iloc[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CASE_NUMBER': '[]',\n",
       " 'COURT': '[]',\n",
       " 'DATE': \"['December 23, 2004']\",\n",
       " 'GPE': '[]',\n",
       " 'JUDGE': '[]',\n",
       " 'LAWYER': '[]',\n",
       " 'ORG': '[]',\n",
       " 'OTHER_PERSON': '[]',\n",
       " 'PETITIONER': '[]',\n",
       " 'PRECEDENT': '[]',\n",
       " 'PROVISION': \"['Rule 141']\",\n",
       " 'RESPONDENT': '[]',\n",
       " 'STATUTE': \"['West Bengal Motor Vehicles Rules']\",\n",
       " 'WITNESS': '[]'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data['entities_dict'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9895, 1100, 1074)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dev_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_col(row):\n",
    "    instruction = \"You are solving the NER problem in indian legal documents. You have to extract from the text, entities related to each of the following categories: CASE_NUMBER, COURT, DATE, GPE, JUDGE, LAWYER, ORG, OTHER_PERSON, PETITIONER, PRECEDENT, PROVISION, RESPONDENT, STATUTE, WITNESS. Extract them exactly as they are in the text (Don't format them). Your output always should be a dictionary in a json readable format (category: list of entities).\"\n",
    "    text_row = f\"\"\"<s> [INST] {instruction} Find the entities in the following text: {row['sentence']} [/INST]\\n {row['raw_entities']} </s>\"\"\"\n",
    "    return text_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmed Shafqat\\AppData\\Local\\Temp\\ipykernel_25992\\1614372929.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['text'] = train_data.apply(create_text_col, axis=1)\n",
      "C:\\Users\\Ahmed Shafqat\\AppData\\Local\\Temp\\ipykernel_25992\\1614372929.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dev_data['text'] = dev_data.apply(create_text_col, axis=1)\n",
      "C:\\Users\\Ahmed Shafqat\\AppData\\Local\\Temp\\ipykernel_25992\\1614372929.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data['text'] = test_data.apply(create_text_col, axis=1)\n"
     ]
    }
   ],
   "source": [
    "train_data['text'] = train_data.apply(create_text_col, axis=1)\n",
    "dev_data['text'] = dev_data.apply(create_text_col, axis=1)\n",
    "test_data['text'] = test_data.apply(create_text_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['train'] = train_data['raw_entities']\n",
    "# dev_data['train'] = dev_data['raw_entities']\n",
    "# test_data['train'] = test_data['raw_entities']\n",
    "\n",
    "# train_data['test'] = train_data['raw_entities']\n",
    "# dev_data['test'] = dev_data['raw_entities']\n",
    "# test_data['test'] = test_data['raw_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['sentence', 'raw_entities', 'entities_dict', 'text']\n",
    "train_data = train_data[selected_columns]\n",
    "dev_data = dev_data[selected_columns]\n",
    "test_data = test_data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CASE_NUMBER': '[]',\n",
       " 'COURT': \"['High Court Of Delhi At New Delhi']\",\n",
       " 'DATE': '[]',\n",
       " 'GPE': '[]',\n",
       " 'JUDGE': \"['Najmi Waziri']\",\n",
       " 'LAWYER': \"['S.P. Jain', 'Himanshu Gambhir', 'Nar Singh', 'Pushkar Singh Kanwal', 'Arvind Chaudhary', 'Ram Kawar', 'Amit Kumar']\",\n",
       " 'ORG': '[]',\n",
       " 'OTHER_PERSON': '[]',\n",
       " 'PETITIONER': \"['Oriental Insurance Co Ltd.']\",\n",
       " 'PRECEDENT': '[]',\n",
       " 'PROVISION': '[]',\n",
       " 'RESPONDENT': \"['Zaixhu Xie', 'Qualcomm India Pvt Ltd']\",\n",
       " 'STATUTE': '[]',\n",
       " 'WITNESS': '[]'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(test_data['raw_entities'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> [INST] You are solving the NER problem in indian legal documents. You have to extract from the text, entities related to each of the following categories: CASE_NUMBER, COURT, DATE, GPE, JUDGE, LAWYER, ORG, OTHER_PERSON, PETITIONER, PRECEDENT, PROVISION, RESPONDENT, STATUTE, WITNESS. Extract them exactly as they are in the text (Don\\'t format them). Your output always should be a dictionary in a json readable format (category: list of entities). Find the entities in the following text: $~40 * In The High Court Of Delhi At New Delhi % Decided on: 31.07.2019 + Mac.App. 976/2018 & Cm Nos. 46122/2018, 15243/2019, 34195/2019 Oriental Insurance Co Ltd. ..... Appellant Through: Mr. S.P. Jain, Mr. Himanshu Gambhir, Mr. Nar Singh and Mr. Pushkar Singh Kanwal, Advocates. Versus Zaixhu Xie & Ors (M/S Qualcomm India Pvt Ltd ) ..... Respondents Through: Mr. Arvind Chaudhary, Advocate for Respondent Nos. 1& 2. Mr. Ram Kawar, Advocate for Mr. Amit Kumar Gupta, Advocate for Respondent No.4. Coram: Hon\\'Ble Mr. Justice Najmi Waziri Najmi Waziri, J. (Oral) [/INST]\\n {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[\\'High Court Of Delhi At New Delhi\\']\", \"DATE\": \"[]\", \"GPE\": \"[]\", \"JUDGE\": \"[\\'Najmi Waziri\\']\", \"LAWYER\": \"[\\'S.P. Jain\\', \\'Himanshu Gambhir\\', \\'Nar Singh\\', \\'Pushkar Singh Kanwal\\', \\'Arvind Chaudhary\\', \\'Ram Kawar\\', \\'Amit Kumar\\']\", \"ORG\": \"[]\", \"OTHER_PERSON\": \"[]\", \"PETITIONER\": \"[\\'Oriental Insurance Co Ltd.\\']\", \"PRECEDENT\": \"[]\", \"PROVISION\": \"[]\", \"RESPONDENT\": \"[\\'Zaixhu Xie\\', \\'Qualcomm India Pvt Ltd\\']\", \"STATUTE\": \"[]\", \"WITNESS\": \"[]\"} </s>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Data/Finetuning/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data.to_csv(path+'train.csv', index=False)\n",
    "dev_data.to_csv(path+'dev.csv', index=False)\n",
    "test_data.to_csv(path+'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import ast\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "categories = ['CASE_NUMBER', 'COURT', 'DATE', 'GPE', 'JUDGE', 'LAWYER', 'ORG', 'OTHER_PERSON', 'PETITIONER', 'PRECEDENT', 'PROVISION', 'RESPONDENT', 'STATUTE', 'WITNESS']\n",
    "def tokenize_and_tag(df: pd.DataFrame, categories: List[str]) -> pd.DataFrame:\n",
    "    # Define tag prefixes\n",
    "    B_PREFIX = 'B-'\n",
    "    I_PREFIX = 'I-'\n",
    "    O_TAG = 'O'\n",
    "\n",
    "    # Prepare output data\n",
    "    output_data = {'tokens': [], 'ner_tags': []}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        entities = row['entities_dict']\n",
    "        # print(entities)\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        # tokens = sentence.split()  # Simple tokenization, can be replaced with a more robust tokenizer\n",
    "        doc = nlp(sentence)\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        # Initialize tags as 'Outside' for each token\n",
    "        tags = [O_TAG for _ in tokens]\n",
    "\n",
    "        # Update tags based on entities\n",
    "        for category, entity_list in entities.items():\n",
    "            entity_lista = ast.literal_eval(entity_list)\n",
    "            for entity in entity_lista:\n",
    "                entity_tokens = entity.split()\n",
    "                # Find all occurrences of the entity in the tokens\n",
    "                for i in range(len(tokens)):\n",
    "                    # print(entity_tokens, tokens[i:i+len(entity_tokens)])\n",
    "                    if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                        # Update the tags for this occurrence of the entity\n",
    "                        tags[i] = B_PREFIX + category\n",
    "                        for j in range(i + 1, i + len(entity_tokens)):\n",
    "                            tags[j] = I_PREFIX + category\n",
    "\n",
    "        output_data['tokens'].append(tokens)\n",
    "        output_data['ner_tags'].append(tags)\n",
    "\n",
    "    return pd.DataFrame(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prova = tokenize_and_tag(test_data, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data['entities_dict'].iloc[0])\n",
    "list(zip(test_prova['tokens'].iloc[0], test_prova['ner_tags'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing f1 score from mistral model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"./results/model_results_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GroundTruth</th>\n",
       "      <th>ModelOutput</th>\n",
       "      <th>ExecutionTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>8.500527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>7.398751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>9.192415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>6.812049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>9.018653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>{\"CASE_NUMBER\": \"['O.S.No.31/2009']\", \"COURT\":...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>7.881613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>{\"CASE_NUMBER\": \"['F.C.O.P.No.41 of 2012']\", \"...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>9.644887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>{\"CASE_NUMBER\": \"['Special Case (NDPS) No.17 o...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>9.620881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>7.973880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>[INST] [INST] You are solving the NER problem ...</td>\n",
       "      <td>6.491162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           GroundTruth   \n",
       "0    {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...  \\\n",
       "1    {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...   \n",
       "2    {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...   \n",
       "3    {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...   \n",
       "4    {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...   \n",
       "..                                                 ...   \n",
       "532  {\"CASE_NUMBER\": \"['O.S.No.31/2009']\", \"COURT\":...   \n",
       "533  {\"CASE_NUMBER\": \"['F.C.O.P.No.41 of 2012']\", \"...   \n",
       "534  {\"CASE_NUMBER\": \"['Special Case (NDPS) No.17 o...   \n",
       "535  {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...   \n",
       "536  {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...   \n",
       "\n",
       "                                           ModelOutput  ExecutionTime  \n",
       "0    [INST] [INST] You are solving the NER problem ...       8.500527  \n",
       "1    [INST] [INST] You are solving the NER problem ...       7.398751  \n",
       "2    [INST] [INST] You are solving the NER problem ...       9.192415  \n",
       "3    [INST] [INST] You are solving the NER problem ...       6.812049  \n",
       "4    [INST] [INST] You are solving the NER problem ...       9.018653  \n",
       "..                                                 ...            ...  \n",
       "532  [INST] [INST] You are solving the NER problem ...       7.881613  \n",
       "533  [INST] [INST] You are solving the NER problem ...       9.644887  \n",
       "534  [INST] [INST] You are solving the NER problem ...       9.620881  \n",
       "535  [INST] [INST] You are solving the NER problem ...       7.973880  \n",
       "536  [INST] [INST] You are solving the NER problem ...       6.491162  \n",
       "\n",
       "[537 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prova = pd.read_csv('./Data/Finetuning/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[\\'Supreme Court of India\\']\", \"DATE\": \"[]\", \"GPE\": \"[]\", \"JUDGE\": \"[]\", \"LAWYER\": \"[]\", \"ORG\": \"[]\", \"OTHER_PERSON\": \"[]\", \"PETITIONER\": \"[]\", \"PRECEDENT\": \"[\\'Satya Pal Anand VS State of M.P & Ors. (2016)10 SCC 767\\']\", \"PROVISION\": \"[]\", \"RESPONDENT\": \"[]\", \"STATUTE\": \"[\\'Registration Act 1908\\']\", \"WITNESS\": \"[]\"}'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prova['raw_entities'].iloc[587]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"[]\", \"GPE\": \"[]\", \"JUDGE\": \"[]\", \"LAWYER\": \"[]\", \"ORG\": \"[]\", \"OTHER_PERSON\": \"[]\", \"PETITIONER\": \"[]\", \"PRECEDENT\": \"[\\'Shambhu Investment (P.) Ltd. vs. CIT (2003) 129 Taxman 70 (Supreme Court)\\', \\'CIT vs. Shambhu Investment (P.) Ltd. \\\\\"(2001) 116 Taxman 795 (Calcutta High Court)\\']\", \"PROVISION\": \"[]\", \"RESPONDENT\": \"[]\", \"STATUTE\": \"[]\", \"WITNESS\": \"[]\"}'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results['GroundTruth'].str.contains(\"CIT vs. Shambhu Investment\")]['GroundTruth'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Invalid \\escape: line 1 column 178 (char 177)\n",
      "corrected string:  {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"[]\", \"GPE\": \"[]\", \"JUDGE\": \"[]\", \"LAWYER\": \"[]\", \"ORG\": \"[]\", \"OTHER_PERSON\": \"[]\", \"PETITIONER\": \"[]\", \"PRECEDENT\": \"[\\\\'Haldar\\\\\\'s cave and Bajoria\\\\\\'s case\\\\']\", \"PROVISION\": \"[]\", \"RESPONDENT\": \"[]\", \"STATUTE\": \"[]\", \"WITNESS\": \"[]\"}\n",
      "Problematic string:  {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"[]\", \"GPE\": \"[]\", \"JUDGE\": \"[]\", \"LAWYER\": \"[]\", \"ORG\": \"[]\", \"OTHER_PERSON\": \"[]\", \"PETITIONER\": \"[]\", \"PRECEDENT\": \"['Haldar\\'s cave and Bajoria\\'s case']\", \"PROVISION\": \"[]\", \"RESPONDENT\": \"[]\", \"STATUTE\": \"[]\", \"WITNESS\": \"[]\"}\n",
      "Error: Invalid \\escape: line 1 column 263 (char 262)\n",
      "corrected string:  {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"[]\", \"GPE\": \"[]\", \"JUDGE\": \"[]\", \"LAWYER\": \"[]\", \"ORG\": \"[]\", \"OTHER_PERSON\": \"[]\", \"PETITIONER\": \"[]\", \"PRECEDENT\": \"[\\\\'Pioneer Hi-Bred\\\\']\", \"PROVISION\": \"[]\", \"RESPONDENT\": \"[]\", \"STATUTE\": \"[\\\\'Plant Breeders\\\\\\' Rights Act\\\\']\", \"WITNESS\": \"[]\"}\n",
      "Problematic string:  {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"[]\", \"GPE\": \"[]\", \"JUDGE\": \"[]\", \"LAWYER\": \"[]\", \"ORG\": \"[]\", \"OTHER_PERSON\": \"[]\", \"PETITIONER\": \"[]\", \"PRECEDENT\": \"['Pioneer Hi-Bred']\", \"PROVISION\": \"[]\", \"RESPONDENT\": \"[]\", \"STATUTE\": \"['Plant Breeders\\' Rights Act']\", \"WITNESS\": \"[]\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Function to correct syntax errors\n",
    "def correct_syntax_errors(string):\n",
    "    # corrected_string = string.replace(\"'\", \"\\'\")\n",
    "\n",
    "    # Find all list-like structures\n",
    "    lists = re.findall(r'(\\[[^\\]]*\\])', string)\n",
    "    \n",
    "    for lst in lists:\n",
    "        # Replace single quotes with escaped single quotes\n",
    "        replaced_list = lst.replace(\"'\", \"\\\\'\")\n",
    "        # Replace the original list with the modified list in the string\n",
    "        string = string.replace(lst, replaced_list)\n",
    "\n",
    "    # Regular expression to find strings within lists\n",
    "    pattern = r\"(\\[[^\\]]*\\])\"\n",
    "\n",
    "    corrected_string = string\n",
    "\n",
    "    # Function to replace quotes in each matched string\n",
    "    def replace_quotes(match):\n",
    "        # Get the matched string\n",
    "        matched_string = match.group(0)\n",
    "        # Replace single and double quotes\n",
    "        replaced_string = matched_string.replace(\"'\", \"\\\\'\").replace('\"', '\\\\\"')\n",
    "        return replaced_string\n",
    "\n",
    "    # Use regex to find and replace quotes in lists\n",
    "    corrected_string = re.sub(pattern, replace_quotes, corrected_string)\n",
    "    # Replace semicolons with commas\n",
    "    # corrected_string = string\n",
    "\n",
    "    # corrected_string = corrected_string.replace('\\\\', '\\\\\\\\')\n",
    "\n",
    "    corrected_string = corrected_string.replace(';', ',')\n",
    "    # corrected_string = string.replace(';', ',')\n",
    "\n",
    "    # Escape single backslashes not used for valid escape sequences\n",
    "    # corrected_string = re.sub(r'\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', corrected_string)\n",
    "\n",
    "    # print(f\"corrected_string: {corrected_string}\")\n",
    "\n",
    "    return corrected_string\n",
    "\n",
    "def parse_json_string(json_str):\n",
    "    try:\n",
    "        # Replace single quotes with double quotes for valid JSON, if necessary\n",
    "        corrected_string = correct_syntax_errors(json_str)\n",
    "        return json.loads(corrected_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # Print the error and the problematic string for inspection\n",
    "        print(f\"Error: {e}\")\n",
    "        print(f\"corrected string: {corrected_string}\")\n",
    "        print(f\"Problematic string: {json_str}\")\n",
    "        return None\n",
    "\n",
    "def extract_ground_truth_dict(row):\n",
    "    # Extract and parse the JSON string from GroundTruth\n",
    "    return parse_json_string(row['GroundTruth'])\n",
    "\n",
    "def extract_model_output_dict(row):\n",
    "    # Extract and parse the JSON string from ModelOutput\n",
    "    model_output_part = row['ModelOutput'].split(\"[/INST]\\n\")[-1]\n",
    "    return parse_json_string(model_output_part)\n",
    "\n",
    "# Apply the functions to each row to create new columns\n",
    "# results['GroundTruthDict'] = results.apply(extract_ground_truth_dict, axis=1)\n",
    "results['ModelOutputDict'] = results.apply(extract_model_output_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
