{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "# from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load spaCy's language model if necessary (for example, the English model)\n",
    "nlp = spacy.blank(\"en\")  # Replace \"en\" with the appropriate language code\n",
    "\n",
    "# Load your .spacy file\n",
    "train_doc_bin = DocBin().from_disk(\"data/train.spacy\")\n",
    "dev_doc_bin = DocBin().from_disk(\"data/dev.spacy\")\n",
    "# Deserialize the docs\n",
    "train_docs = list(train_doc_bin.get_docs(nlp.vocab))\n",
    "dev_docs = list(dev_doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can work with the docs\n",
    "for doc in train_docs[2:3]:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabelsCounts(docs):\n",
    "    labels = []\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            labels.append(ent.label_)\n",
    "\n",
    "    # Convert the list of labels to a NumPy array\n",
    "    labels_array = np.array(labels)\n",
    "    unique_labels, counts = np.unique(labels_array, return_counts=True)\n",
    "    counts = dict(zip(unique_labels, counts))\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLabelsCounts = getLabelsCounts(train_docs)\n",
    "devLabelsCounts = getLabelsCounts(dev_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveLabelsPie(LabelsCounts, name):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(LabelsCounts)))\n",
    "    patches, texts, autotexts = plt.pie(LabelsCounts.values(), labels=LabelsCounts.keys(), \n",
    "            autopct='%1.1f%%', \n",
    "            colors=colors, \n",
    "            startangle=60,\n",
    "            wedgeprops=dict(edgecolor='w'))\n",
    "    for text in texts + autotexts:\n",
    "        text.set_fontsize(9)\n",
    "    plt.axis('equal')  # Equal aspect ratio ensures the pie chart is circular.\n",
    "    plt.title(name, pad=30, fontdict = {'fontsize':20, 'fontstyle' : 'oblique'})\n",
    "    plt.savefig(f\"./plots/{name}.png\", bbox_inches='tight', transparent=True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLabelsPie(trainLabelsCounts, \"Named entity proportions in training\")\n",
    "saveLabelsPie(devLabelsCounts, \"Named entity proportions in development\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train and dev csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# categories = \n",
    "\n",
    "def format_text(text):\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Strip leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Load your .spacy file\n",
    "def load_spacy_file(file_path):\n",
    "    nlp = spacy.blank(\"en\")  # replace \"en\" with your model's language if different\n",
    "    docs = DocBin().from_disk(file_path)\n",
    "    return list(docs.get_docs(nlp.vocab))\n",
    "    # return list(nlp.from_disk(file_path))\n",
    "\n",
    "# Process documents and extract entities\n",
    "# def process_docs(docs):\n",
    "#     data = []\n",
    "#     for doc in docs:\n",
    "#         text = doc.text\n",
    "#         entities = defaultdict(set)\n",
    "#         for ent in doc.ents:\n",
    "#             entities[ent.label_].add(format_text(ent.text))\n",
    "#         entities = {label: list(ents) for label, ents in entities.items()}\n",
    "#         data.append([text, entities])\n",
    "#     return data\n",
    "\n",
    "\n",
    "def process_docs(docs):\n",
    "    data = []\n",
    "    for doc in docs:\n",
    "        text = doc.text\n",
    "        # Using a dict to maintain insertion order and uniqueness\n",
    "        entities = defaultdict(dict)\n",
    "        for ent in doc.ents:\n",
    "            entities[ent.label_][format_text(ent.text)] = None  # Key is the entity, value is a placeholder\n",
    "        # Extracting the keys (unique entities) from each dictionary\n",
    "        entities = {label: list(ents.keys()) for label, ents in entities.items()}\n",
    "        data.append([text, entities])\n",
    "    return data\n",
    "\n",
    "# Convert to DataFrame\n",
    "def to_dataframe(data):\n",
    "    # Find all unique entity labels\n",
    "    all_labels = set()\n",
    "    for _, entities in data:\n",
    "        all_labels.update(entities.keys())\n",
    "    all_labels = sorted(all_labels)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_data = []\n",
    "    for text, entities in data:\n",
    "        row = [format_text(text)] + [entities.get(label, []) for label in all_labels]\n",
    "        df_data.append(row)\n",
    "\n",
    "    columns = ['sentence'] + all_labels\n",
    "    return pd.DataFrame(df_data, columns=columns)\n",
    "\n",
    "# Load data\n",
    "train_docs = load_spacy_file('data/train.spacy')\n",
    "dev_docs = load_spacy_file('data/dev.spacy')\n",
    "\n",
    "# Process documents\n",
    "train_data = process_docs(train_docs)\n",
    "dev_data = process_docs(dev_docs)\n",
    "\n",
    "# Convert to DataFrame\n",
    "train_df = to_dataframe(train_data)\n",
    "dev_df = to_dataframe(dev_data)\n",
    "\n",
    "# Export to CSV (optional)\n",
    "train_df.to_csv('./data/raw/train_data.csv', index=False)\n",
    "dev_df.to_csv('./data/raw/dev_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/raw/train_data.csv\")\n",
    "test = pd.read_csv(\"./data/raw/dev_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = train.sample(frac=0.1, random_state=42) # random_state for reproducibility\n",
    "train = train.drop(dev.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test['sentence'].iloc[900])\n",
    "print(train.iloc[900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev['sentence'].iloc[-4])\n",
    "print(dev.iloc[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset that contains the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_entities_column(df):\n",
    "    def entities_to_string(row):\n",
    "        # Build a dictionary of non-empty entity categories\n",
    "        entities_dict = {category: entities for category, entities in row.items() if category != 'sentence' and \"entities\" not in category }\n",
    "        # Convert the dictionary to a JSON string\n",
    "        return json.dumps(entities_dict)\n",
    "\n",
    "    # Apply the function to each row and create the new column\n",
    "    df['raw_entities'] = df.apply(entities_to_string, axis=1)\n",
    "    return df\n",
    "\n",
    "def create_dict_column(df):\n",
    "    # Function to convert a JSON string to a dictionary\n",
    "    def string_to_dict(json_str):\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return {}  # Returns an empty dictionary in case of a decoding error\n",
    "\n",
    "    # Apply the function to the 'raw_entities' column to create a new dictionary column\n",
    "    df['entities_dict'] = df['raw_entities'].apply(string_to_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_raw_entities_column(train)\n",
    "dev_data = create_raw_entities_column(dev)\n",
    "test_data = create_raw_entities_column(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = create_dict_column(train_data)\n",
    "\n",
    "dev_data = create_dict_column(dev_data)\n",
    "\n",
    "test_data = create_dict_column(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['raw_entities'].iloc[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data['entities_dict'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data), len(dev_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_col(row):\n",
    "    instruction = \"You are solving the NER problem in indian legal documents. You have to extract from the text, entities related to each of the following categories: CASE_NUMBER, COURT, DATE, GPE, JUDGE, LAWYER, ORG, OTHER_PERSON, PETITIONER, PRECEDENT, PROVISION, RESPONDENT, STATUTE, WITNESS. Extract them exactly as they are in the text (Don't format them). Your output always should be a dictionary in a json readable format (category: list of entities).\"\n",
    "    text_row = f\"\"\"<s> [INST] {instruction} Find the entities in the following text: {row['sentence']} [/INST]\\n {row['raw_entities']} </s>\"\"\"\n",
    "    return text_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data.apply(create_text_col, axis=1)\n",
    "dev_data['text'] = dev_data.apply(create_text_col, axis=1)\n",
    "test_data['text'] = test_data.apply(create_text_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['train'] = train_data['raw_entities']\n",
    "# dev_data['train'] = dev_data['raw_entities']\n",
    "# test_data['train'] = test_data['raw_entities']\n",
    "\n",
    "# train_data['test'] = train_data['raw_entities']\n",
    "# dev_data['test'] = dev_data['raw_entities']\n",
    "# test_data['test'] = test_data['raw_entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['sentence', 'raw_entities', 'entities_dict', 'text']\n",
    "train_data = train_data[selected_columns]\n",
    "dev_data = dev_data[selected_columns]\n",
    "test_data = test_data[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(test_data['raw_entities'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Data/Finetuning/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data.to_csv(path+'train.csv', index=False)\n",
    "dev_data.to_csv(path+'dev.csv', index=False)\n",
    "test_data.to_csv(path+'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import ast\n",
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "categories = ['CASE_NUMBER', 'COURT', 'DATE', 'GPE', 'JUDGE', 'LAWYER', 'ORG', 'OTHER_PERSON', 'PETITIONER', 'PRECEDENT', 'PROVISION', 'RESPONDENT', 'STATUTE', 'WITNESS']\n",
    "def tokenize_and_tag(df: pd.DataFrame, categories: List[str]) -> pd.DataFrame:\n",
    "    # Define tag prefixes\n",
    "    B_PREFIX = 'B-'\n",
    "    I_PREFIX = 'I-'\n",
    "    O_TAG = 'O'\n",
    "\n",
    "    # Prepare output data\n",
    "    output_data = {'tokens': [], 'ner_tags': []}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        entities = row['entities_dict']\n",
    "        # print(entities)\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        # tokens = sentence.split()  # Simple tokenization, can be replaced with a more robust tokenizer\n",
    "        doc = nlp(sentence)\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        # Initialize tags as 'Outside' for each token\n",
    "        tags = [O_TAG for _ in tokens]\n",
    "\n",
    "        # Update tags based on entities\n",
    "        for category, entity_list in entities.items():\n",
    "            entity_lista = ast.literal_eval(entity_list)\n",
    "            for entity in entity_lista:\n",
    "                entity_tokens = entity.split()\n",
    "                # Find all occurrences of the entity in the tokens\n",
    "                for i in range(len(tokens)):\n",
    "                    # print(entity_tokens, tokens[i:i+len(entity_tokens)])\n",
    "                    if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                        # Update the tags for this occurrence of the entity\n",
    "                        tags[i] = B_PREFIX + category\n",
    "                        for j in range(i + 1, i + len(entity_tokens)):\n",
    "                            tags[j] = I_PREFIX + category\n",
    "\n",
    "        output_data['tokens'].append(tokens)\n",
    "        output_data['ner_tags'].append(tags)\n",
    "\n",
    "    return pd.DataFrame(output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prova = tokenize_and_tag(test_data, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data['entities_dict'].iloc[0])\n",
    "list(zip(test_prova['tokens'].iloc[0], test_prova['ner_tags'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing f1 score from mistral model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['CASE_NUMBER', 'COURT', 'DATE', 'GPE', 'JUDGE', 'LAWYER', 'ORG', 'OTHER_PERSON', 'PETITIONER', 'PRECEDENT', 'PROVISION', 'RESPONDENT', 'STATUTE', 'WITNESS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to correct syntax errors\n",
    "def correct_syntax_errors(string):\n",
    "    corrected_string = string\n",
    "    corrected_string = corrected_string.replace(\"\\\\'\", \"\\\\\\\\'\")\n",
    "    corrected_string = corrected_string.replace(';', ',')\n",
    "    return corrected_string\n",
    "\n",
    "def parse_json_string(json_str):\n",
    "    try:\n",
    "        corrected_string = correct_syntax_errors(json_str)\n",
    "        return True, json.loads(corrected_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # print(f\"Error: {e}\")\n",
    "        # print(f\"corrected string: {corrected_string}\")\n",
    "        print(f\"Problematic string: {json_str}\")\n",
    "        return False, None\n",
    "\n",
    "def extract_ground_truth_dict(row):\n",
    "    # Extract and parse the JSON string from GroundTruth\n",
    "    success, result = parse_json_string(row['GroundTruth'])\n",
    "    return result if success else None\n",
    "\n",
    "def check_categories(dictionary):\n",
    "    for categ in categories:\n",
    "        if categ not in dictionary.keys():\n",
    "            dictionary[categ] = \"[]\"\n",
    "    return dictionary\n",
    "\n",
    "def extract_model_output_dict(row):\n",
    "    # Extract and parse the JSON string from ModelOutput\n",
    "    model_output = row['ModelOutput']\n",
    "    model_output_part = model_output.replace(\"\\r\", \"\").split(\"[/INST]\\n\")[-1]\n",
    "    success, result = parse_json_string(model_output_part)\n",
    "    if success:\n",
    "        result = check_categories(result)\n",
    "    return result if success else 'drop'\n",
    "\n",
    "def parseRawOutput(results):\n",
    "# Apply the functions to each row to create new columns\n",
    "    results['GroundTruthDict'] = results.apply(extract_ground_truth_dict, axis=1)\n",
    "    results['ModelOutputDict'] = results.apply(extract_model_output_dict, axis=1)\n",
    "    # Drop rows where either column has 'drop' value\n",
    "    dropped = results[(results['ModelOutputDict'] == 'drop') | (results['GroundTruthDict'] == 'drop')].index\n",
    "    print(f\"dropped {len(dropped)} rows due to parsing errors\")\n",
    "    results = results[(results['ModelOutputDict'] != 'drop') & (results['GroundTruthDict'] != 'drop')]\n",
    "    return results, dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = pd.read_csv(\"./results/model_results_2.csv\")\n",
    "results2 = pd.read_csv(\"./results/model_results_1.csv\")\n",
    "results = pd.concat([results1, results2], axis=0)\n",
    "results.reset_index(drop=True)\n",
    "results.to_csv(\"./results/model_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\"./results/model_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cleaned, dropped = parseRawOutput(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute F1 score for a category\n",
    "def f1_score_category(truth, prediction):\n",
    "    truth = set(truth)\n",
    "    prediction = set(prediction)\n",
    "    if truth == prediction and len(prediction) == 0:\n",
    "        tp = 1\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "    else:\n",
    "        tp = len(truth & prediction)\n",
    "        fp = len(prediction - truth)\n",
    "        fn = len(truth - prediction)\n",
    "    # print(truth, prediction, tp, fp, fn)\n",
    "\n",
    "    precision = tp / (tp + fp) if tp + fp != 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn != 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "    return f1\n",
    "\n",
    "def createDictionary(dictionary):\n",
    "    new_dict = {}\n",
    "    for category in dictionary:\n",
    "        try:\n",
    "            new_dict[category] = ast.literal_eval(dictionary[category])\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Return None if ast.literal_eval fails\n",
    "            return None\n",
    "    return new_dict\n",
    "\n",
    "# Function to process a row and compute F1 scores\n",
    "def process_row(row):\n",
    "    ground_truth = createDictionary(row['GroundTruthDict'])\n",
    "    model_output = createDictionary(row['ModelOutputDict'])\n",
    "    # Check if ast.literal_eval failed for either ground_truth or model_output\n",
    "    if ground_truth is None or model_output is None:\n",
    "        return None\n",
    "    f1_scores = {}\n",
    "    for category in ground_truth:\n",
    "        gt_list = ground_truth[category]\n",
    "        model_list = model_output[category]\n",
    "        f1_scores[category] = f1_score_category(gt_list, model_list)\n",
    "    return f1_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(df):\n",
    "    # Apply the function to each row and aggregate results\n",
    "    category_f1_scores = defaultdict(list)\n",
    "    indices_to_drop = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        row_scores = process_row(row)\n",
    "\n",
    "        # Skip rows where process_row returns None\n",
    "        if row_scores is None:\n",
    "            indices_to_drop.append(index)\n",
    "            continue\n",
    "\n",
    "        for category, score in row_scores.items():\n",
    "            category_f1_scores[category].append(score)\n",
    "\n",
    "    # Calculate average F1 scores for each category and macro F1 score for the dataset\n",
    "    average_f1_scores = {category: sum(scores) / len(scores) for category, scores in category_f1_scores.items()}\n",
    "    macro_f1_score = sum(average_f1_scores.values()) / len(average_f1_scores)\n",
    "\n",
    "    print(\"Average F1 Scores by Category:\", average_f1_scores)\n",
    "    print(\"Macro F1 Score for the Dataset:\", macro_f1_score)\n",
    "    return average_f1_scores, macro_f1_score, indices_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_f1_scores, macro_f1_score, indices_to_drop = compute_f1_score(results_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cleaned = results_cleaned.drop(index=indices_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(results_cleaned.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
