{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "# from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import Dataset, DatasetDict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['CASE_NUMBER', 'COURT', 'DATE', 'GPE', 'JUDGE', 'LAWYER', 'ORG', 'OTHER_PERSON', 'PETITIONER', 'PRECEDENT', 'PROVISION', 'RESPONDENT', 'STATUTE', 'WITNESS']\n",
    "\n",
    "# Create label2id and id2label dictionaries\n",
    "B_PREFIX = 'B-'\n",
    "I_PREFIX = 'I-'\n",
    "O_TAG = 'O'\n",
    "label2id = {O_TAG: 0}\n",
    "id2label = {0: O_TAG}\n",
    "idx = 1\n",
    "for category in categories:\n",
    "    label2id[B_PREFIX + category] = idx\n",
    "    id2label[idx] = B_PREFIX + category\n",
    "    idx += 1\n",
    "    label2id[I_PREFIX + category] = idx\n",
    "    id2label[idx] = I_PREFIX + category\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data [don't run if already created]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/finetuning/train.csv\")\n",
    "dev = pd.read_csv(\"./data/finetuning/dev.csv\")\n",
    "test = pd.read_csv(\"./data/finetuning/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>raw_entities</th>\n",
       "      <th>entities_dict</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$~40 * In The High Court Of Delhi At New Delhi...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"['High Court O...</td>\n",
       "      <td>{'CASE_NUMBER': '[]', 'COURT': \"['High Court O...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 Reportable In The Supreme Court Of India Civ...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"['Supreme Cour...</td>\n",
       "      <td>{'CASE_NUMBER': '[]', 'COURT': \"['Supreme Cour...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R/Scr.A/9089/2017 Judgment In The High Court O...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"['High Court O...</td>\n",
       "      <td>{'CASE_NUMBER': '[]', 'COURT': \"['High Court O...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High Court Of Judicature For Rajasthan Bench A...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"['High Court O...</td>\n",
       "      <td>{'CASE_NUMBER': '[]', 'COURT': \"['High Court O...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 In The High Court Of Judicature At Madras Da...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"['High Court O...</td>\n",
       "      <td>{'CASE_NUMBER': '[]', 'COURT': \"['High Court O...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>Apparently, Channaraddi set up his daughters G...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"['O.S.No.31/2009']\", \"COURT\":...</td>\n",
       "      <td>{'CASE_NUMBER': \"['O.S.No.31/2009']\", 'COURT':...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>After the dismissal of the petition for annulm...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"['F.C.O.P.No.41 of 2012']\", \"...</td>\n",
       "      <td>{'CASE_NUMBER': \"['F.C.O.P.No.41 of 2012']\", '...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>On 12.07.2018, a letter was received from the ...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"['Special Case (NDPS) No.17 o...</td>\n",
       "      <td>{'CASE_NUMBER': \"['Special Case (NDPS) No.17 o...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>The date on which the measurements were record...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>{'CASE_NUMBER': '[]', 'COURT': '[]', 'DATE': '...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>The lower back end was 5 cm behind root of rig...</td>\n",
       "      <td>{\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...</td>\n",
       "      <td>{'CASE_NUMBER': '[]', 'COURT': '[]', 'DATE': '...</td>\n",
       "      <td>&lt;s&gt; [INST] You are solving the NER problem in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1074 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0     $~40 * In The High Court Of Delhi At New Delhi...   \n",
       "1     1 Reportable In The Supreme Court Of India Civ...   \n",
       "2     R/Scr.A/9089/2017 Judgment In The High Court O...   \n",
       "3     High Court Of Judicature For Rajasthan Bench A...   \n",
       "4     1 In The High Court Of Judicature At Madras Da...   \n",
       "...                                                 ...   \n",
       "1069  Apparently, Channaraddi set up his daughters G...   \n",
       "1070  After the dismissal of the petition for annulm...   \n",
       "1071  On 12.07.2018, a letter was received from the ...   \n",
       "1072  The date on which the measurements were record...   \n",
       "1073  The lower back end was 5 cm behind root of rig...   \n",
       "\n",
       "                                           raw_entities  \\\n",
       "0     {\"CASE_NUMBER\": \"[]\", \"COURT\": \"['High Court O...   \n",
       "1     {\"CASE_NUMBER\": \"[]\", \"COURT\": \"['Supreme Cour...   \n",
       "2     {\"CASE_NUMBER\": \"[]\", \"COURT\": \"['High Court O...   \n",
       "3     {\"CASE_NUMBER\": \"[]\", \"COURT\": \"['High Court O...   \n",
       "4     {\"CASE_NUMBER\": \"[]\", \"COURT\": \"['High Court O...   \n",
       "...                                                 ...   \n",
       "1069  {\"CASE_NUMBER\": \"['O.S.No.31/2009']\", \"COURT\":...   \n",
       "1070  {\"CASE_NUMBER\": \"['F.C.O.P.No.41 of 2012']\", \"...   \n",
       "1071  {\"CASE_NUMBER\": \"['Special Case (NDPS) No.17 o...   \n",
       "1072  {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...   \n",
       "1073  {\"CASE_NUMBER\": \"[]\", \"COURT\": \"[]\", \"DATE\": \"...   \n",
       "\n",
       "                                          entities_dict  \\\n",
       "0     {'CASE_NUMBER': '[]', 'COURT': \"['High Court O...   \n",
       "1     {'CASE_NUMBER': '[]', 'COURT': \"['Supreme Cour...   \n",
       "2     {'CASE_NUMBER': '[]', 'COURT': \"['High Court O...   \n",
       "3     {'CASE_NUMBER': '[]', 'COURT': \"['High Court O...   \n",
       "4     {'CASE_NUMBER': '[]', 'COURT': \"['High Court O...   \n",
       "...                                                 ...   \n",
       "1069  {'CASE_NUMBER': \"['O.S.No.31/2009']\", 'COURT':...   \n",
       "1070  {'CASE_NUMBER': \"['F.C.O.P.No.41 of 2012']\", '...   \n",
       "1071  {'CASE_NUMBER': \"['Special Case (NDPS) No.17 o...   \n",
       "1072  {'CASE_NUMBER': '[]', 'COURT': '[]', 'DATE': '...   \n",
       "1073  {'CASE_NUMBER': '[]', 'COURT': '[]', 'DATE': '...   \n",
       "\n",
       "                                                   text  \n",
       "0     <s> [INST] You are solving the NER problem in ...  \n",
       "1     <s> [INST] You are solving the NER problem in ...  \n",
       "2     <s> [INST] You are solving the NER problem in ...  \n",
       "3     <s> [INST] You are solving the NER problem in ...  \n",
       "4     <s> [INST] You are solving the NER problem in ...  \n",
       "...                                                 ...  \n",
       "1069  <s> [INST] You are solving the NER problem in ...  \n",
       "1070  <s> [INST] You are solving the NER problem in ...  \n",
       "1071  <s> [INST] You are solving the NER problem in ...  \n",
       "1072  <s> [INST] You are solving the NER problem in ...  \n",
       "1073  <s> [INST] You are solving the NER problem in ...  \n",
       "\n",
       "[1074 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def tokenize_and_tag(df: pd.DataFrame, categories: List[str]) -> pd.DataFrame:\n",
    "    # Define tag prefixes\n",
    "    B_PREFIX = 'B-'\n",
    "    I_PREFIX = 'I-'\n",
    "    O_TAG = 'O'\n",
    "\n",
    "    # Prepare output data\n",
    "    output_data = {'tokens': [], 'ner_tags': []}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        entities = row['entities_dict']\n",
    "        # print(entities)\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        # tokens = sentence.split()  # Simple tokenization, can be replaced with a more robust tokenizer\n",
    "        doc = nlp(sentence)\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        # Initialize tags as 'Outside' for each token\n",
    "        tags = [O_TAG for _ in tokens]\n",
    "\n",
    "        entities = ast.literal_eval(entities)\n",
    "        # print(type(entities))\n",
    "\n",
    "        # Update tags based on entities\n",
    "        for category, entity_list in entities.items():\n",
    "            entity_lista = ast.literal_eval(entity_list)\n",
    "            for entity in entity_lista:\n",
    "                entity_tokens = entity.split()\n",
    "                # Find all occurrences of the entity in the tokens\n",
    "                for i in range(len(tokens)):\n",
    "                    # print(entity_tokens, tokens[i:i+len(entity_tokens)])\n",
    "                    if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                        # Update the tags for this occurrence of the entity\n",
    "                        tags[i] = B_PREFIX + category\n",
    "                        for j in range(i + 1, i + len(entity_tokens)):\n",
    "                            tags[j] = I_PREFIX + category\n",
    "\n",
    "        output_data['tokens'].append(tokens)\n",
    "        output_data['ner_tags'].append(tags)\n",
    "        data = pd.DataFrame(output_data) \n",
    "        data['ner_tags_str'] = data['ner_tags']\n",
    "        data['ner_tags'] = data['ner_tags'].apply(lambda x: list(map(label2id.get, x)))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenize_and_tag(train, categories)\n",
    "dev_data = tokenize_and_tag(dev, categories)\n",
    "test_data = tokenize_and_tag(test, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"./data/roberta/train.csv\", index=False)\n",
    "dev_data.to_csv(\"./data/roberta/dev.csv\", index=False)\n",
    "test_data.to_csv(\"./data/roberta/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>ner_tags_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(, 7, ), On, specific, query, by, the, Bench,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[He, was, also, asked, whether, Agya, &lt;, span,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[O, O, O, O, O, B-OTHER_PERSON, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5.2, CW3, Mr, Vijay, Mishra, ,, Deputy, Manag...</td>\n",
       "      <td>[0, 0, 0, 27, 28, 0, 0, 0, 0, 13, 14, 0, 0, 0,...</td>\n",
       "      <td>[O, O, O, B-WITNESS, I-WITNESS, O, O, O, O, B-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The, pillion, rider, T.V., Satyanarayana, Mur...</td>\n",
       "      <td>[0, 0, 0, 15, 16, 16, 0, 0, 0, 0]</td>\n",
       "      <td>[O, O, O, B-OTHER_PERSON, I-OTHER_PERSON, I-OT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[,, if, the, argument, of, the, learned, couns...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>[1, ®, In, The, High, Court, Of, Karnataka, At...</td>\n",
       "      <td>[0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, B-COURT, I-COURT, I-COURT, I-COUR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9891</th>\n",
       "      <td>[They, had, admittedly, left, India, after, th...</td>\n",
       "      <td>[0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, B-GPE, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9892</th>\n",
       "      <td>[Non, -, applicant, produced, witnesses, NAW, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 27, 28, 0, 0, 0, 0, 0, 2...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-WITNESS, I-WITNESS, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9893</th>\n",
       "      <td>[No, doubt, ,, civil, and, criminal, jurisdict...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9894</th>\n",
       "      <td>[In, 1994, MPU681, (, M., P., Motor, Yan, Kara...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9895 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0     [(, 7, ), On, specific, query, by, the, Bench,...   \n",
       "1     [He, was, also, asked, whether, Agya, <, span,...   \n",
       "2     [5.2, CW3, Mr, Vijay, Mishra, ,, Deputy, Manag...   \n",
       "3     [The, pillion, rider, T.V., Satyanarayana, Mur...   \n",
       "4     [,, if, the, argument, of, the, learned, couns...   \n",
       "...                                                 ...   \n",
       "9890  [1, ®, In, The, High, Court, Of, Karnataka, At...   \n",
       "9891  [They, had, admittedly, left, India, after, th...   \n",
       "9892  [Non, -, applicant, produced, witnesses, NAW, ...   \n",
       "9893  [No, doubt, ,, civil, and, criminal, jurisdict...   \n",
       "9894  [In, 1994, MPU681, (, M., P., Motor, Yan, Kara...   \n",
       "\n",
       "                                               ner_tags  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1     [0, 0, 0, 0, 0, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2     [0, 0, 0, 27, 28, 0, 0, 0, 0, 13, 14, 0, 0, 0,...   \n",
       "3                     [0, 0, 0, 15, 16, 16, 0, 0, 0, 0]   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "9890  [0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, ...   \n",
       "9891  [0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9892  [0, 0, 0, 0, 0, 0, 0, 27, 28, 0, 0, 0, 0, 0, 2...   \n",
       "9893  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "9894  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                           ner_tags_str  \n",
       "0     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1     [O, O, O, O, O, B-OTHER_PERSON, O, O, O, O, O,...  \n",
       "2     [O, O, O, B-WITNESS, I-WITNESS, O, O, O, O, B-...  \n",
       "3     [O, O, O, B-OTHER_PERSON, I-OTHER_PERSON, I-OT...  \n",
       "4     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "...                                                 ...  \n",
       "9890  [O, O, O, O, B-COURT, I-COURT, I-COURT, I-COUR...  \n",
       "9891  [O, O, O, O, B-GPE, O, O, O, O, O, O, O, O, O,...  \n",
       "9892  [O, O, O, O, O, O, O, B-WITNESS, I-WITNESS, O,...  \n",
       "9893  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "9894  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[9895 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns_to_list(df):\n",
    "    for categ in df.columns:\n",
    "        df[categ] = df[categ].apply(ast.literal_eval)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/roberta/train.csv\")\n",
    "dev_data = pd.read_csv(\"./data/roberta/dev.csv\")\n",
    "test_data = pd.read_csv(\"./data/roberta/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform_columns_to_list(train_data)\n",
    "dev_data = transform_columns_to_list(dev_data)\n",
    "test_data = transform_columns_to_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face's Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "dev_dataset = Dataset.from_pandas(dev_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Create a DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': dev_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 9895\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 1100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'ner_tags_str'],\n",
       "        num_rows: 1074\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bcdc2ca0b94b08ad206aedf62ccbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0073b015e881487495f8f2a5fc35c7c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8e57ac44221437b8de3aeb325bd4877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁(', '▁7', '▁)', '▁On', '▁specific', '▁que', 'ry', '▁by', '▁the', '▁Ben', 'ch', '▁about', '▁an', '▁entry', '▁of', '▁Rs', '▁', '.', '▁1,3', '1', ',', '37', ',', '500', '▁on', '▁deposit', '▁side', '▁of', '▁Hongkong', '▁Bank', '▁account', '▁of', '▁which', '▁a', '▁photo', '▁copy', '▁is', '▁appear', 'ing', '▁at', '▁p', '.', '▁40', '▁of', '▁assess', 'ee', \"▁'\", 's', '▁paper', '▁book', '▁', ',', '▁learned', '▁author', 'ised', '▁representativ', 'e', '▁submitted', '▁that', '▁it', '▁was', '▁related', '▁to', '▁loan', '▁from', '▁broker', '▁', ',', '▁Rahul', '▁&', '▁Co', '.', '▁on', '▁the', '▁basis', '▁of', '▁his', '▁sub', 'mission', '▁a', '▁necessary', '▁mark', '▁is', '▁put', '▁by', '▁us', '▁on', '▁that', '▁photo', '▁copy', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "inputs = data['train'][0]['tokens']\n",
    "inputs = tokenizer(inputs, is_split_into_words=True)\n",
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 30, 31, 31, 32, 33, 34, 34, 35, 35, 36, 37, 38, 38, 39, 40, 40, 41, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 51, 52, 53, 54, 54, 55, 56, 57, 58, 59, 60, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 72, None]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "  new_labels = []\n",
    "  current_word=None\n",
    "  for word_id in word_ids:\n",
    "    if word_id != current_word:\n",
    "      current_word = word_id\n",
    "      label = -100 if word_id is None else labels[word_id]\n",
    "      new_labels.append(label)\n",
    "\n",
    "    elif word_id is None:\n",
    "      new_labels.append(-100)\n",
    "\n",
    "    else:\n",
    "      label = labels[word_id]\n",
    "\n",
    "      if label%2==1:\n",
    "        label = label + 1\n",
    "      new_labels.append(label)\n",
    "\n",
    "  return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [None, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 14, 15, 15, 15, 15, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 30, 31, 31, 32, 33, 34, 34, 35, 35, 36, 37, 38, 38, 39, 40, 40, 41, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 51, 52, 53, 54, 54, 55, 56, 57, 58, 59, 60, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 72, None]\n"
     ]
    }
   ],
   "source": [
    "labels = data['train'][0]['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "  tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "  all_labels = examples['ner_tags']\n",
    "\n",
    "  new_labels = []\n",
    "  for i, labels in enumerate(all_labels):\n",
    "    word_ids = tokenized_inputs.word_ids(i)\n",
    "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "  tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "  return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75501b9386ce45dcab9a91dd145b0a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d640bfd34a4751ae5e8f1528eafc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b140c2aee3141c69bc750acd0c13680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1074 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, remove_columns=data['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9895\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1074\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     0,     15,    361,   1388,   2161,  29458,     41,   1294,    390,\n",
      "             70,   3419,    206,   1672,    142,  42805,    111, 115034,      6,\n",
      "              5,  46963,    418,      4,  10945,      4,   4283,     98,  40370,\n",
      "           5609,    111, 185934,   4932,  15426,    111,   3129,     10,  16186,\n",
      "          43658,     83, 108975,    214,     99,    915,      5,   1112,    111,\n",
      "         202120,   7039,    242,      7,  15122,  12877,      6,      4,  97384,\n",
      "          42179,  52021,  99638,     13, 230121,    450,    442,    509,  62548,\n",
      "             47, 111628,   1295, 115835,      6,      4, 191367,    619,   1311,\n",
      "              5,     98,     70,  18231,    111,   1919,   1614,  21150,     10,\n",
      "          63559,  16188,     83,   3884,    390,   1821,     98,    450,  16186,\n",
      "          43658,      6,      5,      2],\n",
      "        [     0,   1529,    509,   2843,  37170,  36766,  12342,    395,   4426,\n",
      "          27734,  18507,  22422,  15080,    555,    454,  22829,     44,   3447,\n",
      "          22422,  19332,    454,    758,     44,    977,      6, 154791,    438,\n",
      "              5,    363,   4046,     20,  57976,    111,   9571,    305,  42946,\n",
      "          19332,    977,   1136,    474,      6,      4,  42732,     20,     23,\n",
      "             20,  27165,    111,     70,      8,  45710,   5281, 158930,  84797,\n",
      "            538,   1295,   6760,    365,  17032,  25961,      6,      5,      2,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "              1,      1,      1,      1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,   13,   14,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,   13,   14,   14,\n",
      "           14,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,   15,   16,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,   15,   16,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,   15,   16,   16,   16,\n",
      "            0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "metric = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  logits, labels = eval_preds\n",
    "\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "  true_labels = [[id2label[l] for l in label if l!=-100] for label in labels]\n",
    "\n",
    "  true_predictions = [[id2label[p] for p,l in zip(prediction, label) if l!=-100]\n",
    "                      for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "  all_metrics = metric.compute(predictions=true_predictions, references=true_labels, scheme=\"IOB2\", mode=\"strict\", zero_division=0)\n",
    "\n",
    "  return {\"precision\": all_metrics['overall_precision'],\n",
    "          \"recall\": all_metrics['overall_recall'],\n",
    "          \"f1\": all_metrics['overall_f1'],\n",
    "          \"accuracy\": all_metrics['overall_accuracy']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#                                                     model_checkpoint,\n",
    "#                                                     id2label=id2label,\n",
    "#                                                     label2id=label2id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                                                    \"./distilbert-finetuned-ner/checkpoint-3711\",\n",
    "                                                    id2label=id2label,\n",
    "                                                    label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\"distilbert-finetuned-ner\",\n",
    "                         evaluation_strategy = \"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         learning_rate = 2e-5,\n",
    "                         num_train_epochs=3,\n",
    "                         weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset = tokenized_datasets['train'],\n",
    "                  eval_dataset = tokenized_datasets['validation'],\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8369a1e418284849944fbabce0a0ac52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0248, 'learning_rate': 1.7305308542171922e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0233, 'learning_rate': 1.4610617084343843e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea84da579aec45ccb9ff4c897aa6cfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10630408674478531, 'eval_precision': 0.8756989247311828, 'eval_recall': 0.8806228373702422, 'eval_f1': 0.878153978865646, 'eval_accuracy': 0.9812913556408506, 'eval_runtime': 72.6488, 'eval_samples_per_second': 15.141, 'eval_steps_per_second': 1.9, 'epoch': 1.0}\n",
      "{'loss': 0.0186, 'learning_rate': 1.1915925626515765e-05, 'epoch': 1.21}\n",
      "{'loss': 0.0154, 'learning_rate': 9.221234168687686e-06, 'epoch': 1.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da64f5dd5bb54810905c72ac6aa05873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1097690686583519, 'eval_precision': 0.8632193494578816, 'eval_recall': 0.8953287197231834, 'eval_f1': 0.8789808917197451, 'eval_accuracy': 0.9801139708479525, 'eval_runtime': 71.9323, 'eval_samples_per_second': 15.292, 'eval_steps_per_second': 1.918, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
