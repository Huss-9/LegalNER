{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "# from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import Dataset, DatasetDict\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['CASE_NUMBER', 'COURT', 'DATE', 'GPE', 'JUDGE', 'LAWYER', 'ORG', 'OTHER_PERSON', 'PETITIONER', 'PRECEDENT', 'PROVISION', 'RESPONDENT', 'STATUTE', 'WITNESS']\n",
    "\n",
    "# Create label2id and id2label dictionaries\n",
    "B_PREFIX = 'B-'\n",
    "I_PREFIX = 'I-'\n",
    "O_TAG = 'O'\n",
    "label2id = {O_TAG: 0}\n",
    "id2label = {0: O_TAG}\n",
    "idx = 1\n",
    "for category in categories:\n",
    "    label2id[B_PREFIX + category] = idx\n",
    "    id2label[idx] = B_PREFIX + category\n",
    "    idx += 1\n",
    "    label2id[I_PREFIX + category] = idx\n",
    "    id2label[idx] = I_PREFIX + category\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data [don't run if already created]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/finetuning/train.csv\")\n",
    "dev = pd.read_csv(\"./data/finetuning/dev.csv\")\n",
    "test = pd.read_csv(\"./data/finetuning/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def tokenize_and_tag(df: pd.DataFrame, categories: List[str]) -> pd.DataFrame:\n",
    "    # Define tag prefixes\n",
    "    B_PREFIX = 'B-'\n",
    "    I_PREFIX = 'I-'\n",
    "    O_TAG = 'O'\n",
    "\n",
    "    # Prepare output data\n",
    "    output_data = {'tokens': [], 'ner_tags': []}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        entities = row['entities_dict']\n",
    "        # print(entities)\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        # tokens = sentence.split()  # Simple tokenization, can be replaced with a more robust tokenizer\n",
    "        doc = nlp(sentence)\n",
    "        tokens = [token.text for token in doc]\n",
    "\n",
    "        # Initialize tags as 'Outside' for each token\n",
    "        tags = [O_TAG for _ in tokens]\n",
    "\n",
    "        entities = ast.literal_eval(entities)\n",
    "        # print(type(entities))\n",
    "\n",
    "        # Update tags based on entities\n",
    "        for category, entity_list in entities.items():\n",
    "            entity_lista = ast.literal_eval(entity_list)\n",
    "            for entity in entity_lista:\n",
    "                entity_tokens = entity.split()\n",
    "                # Find all occurrences of the entity in the tokens\n",
    "                for i in range(len(tokens)):\n",
    "                    # print(entity_tokens, tokens[i:i+len(entity_tokens)])\n",
    "                    if tokens[i:i+len(entity_tokens)] == entity_tokens:\n",
    "                        # Update the tags for this occurrence of the entity\n",
    "                        tags[i] = B_PREFIX + category\n",
    "                        for j in range(i + 1, i + len(entity_tokens)):\n",
    "                            tags[j] = I_PREFIX + category\n",
    "\n",
    "        output_data['tokens'].append(tokens)\n",
    "        output_data['ner_tags'].append(tags)\n",
    "        data = pd.DataFrame(output_data) \n",
    "        data['ner_tags_str'] = data['ner_tags']\n",
    "        data['ner_tags'] = data['ner_tags'].apply(lambda x: list(map(label2id.get, x)))\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenize_and_tag(train, categories)\n",
    "dev_data = tokenize_and_tag(dev, categories)\n",
    "test_data = tokenize_and_tag(test, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"./data/roberta/train.csv\", index=False)\n",
    "dev_data.to_csv(\"./data/roberta/dev.csv\", index=False)\n",
    "test_data.to_csv(\"./data/roberta/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_columns_to_list(df):\n",
    "    for categ in df.columns:\n",
    "        df[categ] = df[categ].apply(ast.literal_eval)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/roberta/train.csv\")\n",
    "dev_data = pd.read_csv(\"./data/roberta/dev.csv\")\n",
    "test_data = pd.read_csv(\"./data/roberta/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform_columns_to_list(train_data)\n",
    "dev_data = transform_columns_to_list(dev_data)\n",
    "test_data = transform_columns_to_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrames to Hugging Face's Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "dev_dataset = Dataset.from_pandas(dev_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "# Create a DatasetDict\n",
    "data = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': dev_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = data['train'][0]['tokens']\n",
    "inputs = tokenizer(inputs, is_split_into_words=True)\n",
    "print(inputs.tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "  new_labels = []\n",
    "  current_word=None\n",
    "  for word_id in word_ids:\n",
    "    if word_id != current_word:\n",
    "      current_word = word_id\n",
    "      label = -100 if word_id is None else labels[word_id]\n",
    "      new_labels.append(label)\n",
    "\n",
    "    elif word_id is None:\n",
    "      new_labels.append(-100)\n",
    "\n",
    "    else:\n",
    "      label = labels[word_id]\n",
    "\n",
    "      if label%2==1:\n",
    "        label = label + 1\n",
    "      new_labels.append(label)\n",
    "\n",
    "  return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['train'][0]['ner_tags']\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels, word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "  tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "  all_labels = examples['ner_tags']\n",
    "\n",
    "  new_labels = []\n",
    "  for i, labels in enumerate(all_labels):\n",
    "    word_ids = tokenized_inputs.word_ids(i)\n",
    "    new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "  tokenized_inputs['labels'] = new_labels\n",
    "\n",
    "  return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(tokenize_and_align_labels, batched=True, remove_columns=data['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "metric = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "  logits, labels = eval_preds\n",
    "\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "  true_labels = [[id2label[l] for l in label if l!=-100] for label in labels]\n",
    "\n",
    "  true_predictions = [[id2label[p] for p,l in zip(prediction, label) if l!=-100]\n",
    "                      for prediction, label in zip(predictions, labels)]\n",
    "\n",
    "  all_metrics = metric.compute(predictions=true_predictions, references=true_labels, scheme=\"IOB2\", mode=\"strict\", zero_division=0)\n",
    "\n",
    "  return {\"precision\": all_metrics['overall_precision'],\n",
    "          \"recall\": all_metrics['overall_recall'],\n",
    "          \"f1\": all_metrics['overall_f1'],\n",
    "          \"accuracy\": all_metrics['overall_accuracy']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#                                                     model_checkpoint,\n",
    "#                                                     id2label=id2label,\n",
    "#                                                     label2id=label2id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "                                                    \"./distilbert-finetuned-ner/checkpoint-3711\",\n",
    "                                                    id2label=id2label,\n",
    "                                                    label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\"distilbert-finetuned-ner\",\n",
    "                         evaluation_strategy = \"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         learning_rate = 2e-5,\n",
    "                         num_train_epochs=3,\n",
    "                         weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset = tokenized_datasets['train'],\n",
    "                  eval_dataset = tokenized_datasets['validation'],\n",
    "                  data_collator=data_collator,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
